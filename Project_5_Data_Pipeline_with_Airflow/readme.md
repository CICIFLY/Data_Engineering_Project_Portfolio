### Introduction:  
    A music streaming company, Sparkify, wants to introduce more automation and monitoring to their data warehouse ETL pipelines. 
    Apache Airflow is the tool they decided to use. As a data engineer brought into this project, I need to create custom operators
    to perform tasks such as staging the data, filling the data warehouse and running checks on the data as final step. 


### Data sets:
    The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. 
    The source datasets consist of JSON logs that tell about user activity in the application and
    JSON metadata about the songs the users listen to.
    (1). Link to Song data: s3://udacity-dend/song_data
        The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata   
        about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 
  
        For example,here are filepaths to a file in this dataset.    
        song_data/A/B/C/TRABCEI128F424C983.json

        Single song file example :
       {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "",    
       "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
       
     (2). Link to Log data: s3://udacity-dend/log_data
         It consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These 
         simulate app activity logs from an imaginary music streaming app based on configuration settings.     
         The log files are partitioned by year and month. For example, here are filepaths to two files in this dataset.

         log_data/2018/11/2018-11-12-events.json
         log_data/2018/11/2018-11-13-events.json  


### Star Schema for Song Play Analysis ( a star schema picture can be found in this project)
    (1).Fact Table
        songplays(records in event data associated with song plays i.e. records with page NextSong): songplay_id, start_time,
        user_id, level, song_id, artist_id, session_id, location, user_agent
        
    (2).Dimension Tables
        users(users in the app): user_id, first_name, last_name, gender, level
        songs(songs in music database): song_id, title, artist_id, year, duration
        artists(artists in music database): artist_id, name, location, lattitude, longitude
        time(timestamps of records in songplays broken down into specific units): start_time, hour, day, week, month, year, weekday


### Files included in the Repository
     2 folders and 1 file : dags , plugins folders and create_tables.sql file 
   
    (1). The DAG template:
         It has all the imports and task templates in place, but the task dependencies have not been set

    (2).Plugins folder : 
        The operators folder with operator templates :
            data_quality.py   
            load_dimension.py 
            load_fact.py   
            stage_redshift.py       
        A helper class for the SQL transformations  : sql_queries.py
    (3). create_tables.sql :   this file should be converted to a python file with creating and dropping table sql statements
         Pls refer to the projects before 


# Prerequisites
    AWS redshift cluster must run successfully
    read and write access to S3 and aws redshift services
    Tables must be created in Redshift before executing the DAG workflow. The create tables statements can be found in: create_tables.sql ( refer to project 3) 
    
### Steps to run the project
    (1). Launching a redshift cluster
         
    (2). creating tables
        method1: Create Tables by using query editor and the statements from create_tables.sql
        method2: refer to project 3 using 2 python files
    
    (3). Adding airflow connections 
        On the create varaible page,enter the following values 
           Set "Key" equal to "s3_bucket" and set "Val" equal to "udacity-dend"
           Set "Key" equal to "s3_prefix" and set "Val" equal to "data-pipelines"

        On the create connection page, enter the following values:
            Conn Id: Enter aws_credentials.
            Conn Type: Enter Amazon Web Services.
            Login: Enter your Access key ID from the IAM User credentials you downloaded earlier.
            Password: Enter your Secret access key from the IAM User credentials you downloaded earlier.

        Once you've entered these values, select Save and Add Another.
        On the next create connection page, enter the following values:
           Conn Id: Enter redshift.
           Conn Type: Enter Postgres.
           Host: Enter the endpoint of your Redshift cluster, excluding the port at the end. 
           Schema: Enter dev. This is the Redshift database you want to connect to.
           Login: Enter awsuser.
           Password: Enter the password you created when launching your Redshift cluster.
           Port: Enter 5439.
        Once you've entered these values, select Save.

        Everytime you open Udacity workspace to enter Airflow UI, you have to re-create the connections and variable. 
            
    (3). Running the tempalte DAG and compare with the graph provided in the project instruction. 
        Check the logs to see if it has " operator is not implemented" messages
        
    (4). Configuring the DAG
        In the DAG, add default parameters according to these guidelines:
            The DAG does not have dependencies on past runs            
            On failure, the task are retried 3 times
            Retries happen every 5 minutes
            Catchup is turned off
            Do not email on retry
        In the DAG, configure the task dependencies , compare with the graph view with what provided in   
        project instruction.  
        
    (5). Building the operators
        Stage Operator
        The stage operator is expected to be able to load any JSON formatted files from S3 to Amazon Redshift. The operator creates and 
        runs a SQL COPY statement based on the parameters provided. The operator's parameters should specify where in S3 the file is 
        loaded and what is the target table.
        The parameters should be used to distinguish between JSON file. Another important requirement of the stage operator is containing   
        templated field that allows it to load timestamped files from S3 based on the execution time and run backfills.
        
        Fact and Dimension Operators
        With dimension and fact operators, you can utilize the provided SQL helper class to run data transformations. Most of the logic 
        is within the SQL transformations and the operator is expected to take as input a SQL statement and target database on which
        to run the query against. You can also define a target table that will contain the results of the transformation.
        Dimension loads are often done with the truncate-insert pattern where the target table is emptied before the load. Thus, you         
        could also have a parameter that allows switching between insert modes when loading dimensions. Fact tables are usually so 
        massive that they should only allow append type functionality.
        
        Data Quality Operator
        The final operator to create is the data quality operator, which is used to run checks on the data itself. The operator's 
        main functionality is to receive one or more SQL based test cases along with the expected results and execute the tests. 
        For each the test, the test result and expected result needs to be checked and if there is no match, the operator should 
        raise an exception and the task should retry and fail eventually.

        For example one test could be a SQL statement that checks if certain column contains NULL values by counting all the rows
        that have NULL in the column. We do not want to have any NULLs so expected result would be 0 and the test would compare the 
        SQL statement's outcome to the expected result.
        
    (6). Running create_tables.py script to create tables to AWS Redshift
    

### Note about Workspace
    After you have updated the DAG, you will need to run /opt/airflow/start.sh command to start the Airflow web server. Once the Airflow web server is ready, you can access the Airflow UI by clicking on the blue Access Airflow button.
    
    A quality control must be implemented to make sure the workflow can work properly by counting the number of records of each table 

    Command to kill the running DAG or reset Airflow : run command  "pkill -f airflow"  ( This one does not work for me. I have to save all files and reset the data in workspace to refresh it)

    To extract compression files: 
    tar xvzf file.tar.gz   ( by defult, it will be extracted in current working dir)
    
### References :
   (1). https://github.com/gfkw/dend-project-5
   
   (2). https://github.com/jukkakansanaho/udacity-dend-project-5
   
   (3). https://github.com/FedericoSerini/DEND-Project-5-Data-Pipelines
